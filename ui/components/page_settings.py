"""
XY Stock è‚¡ç¥¨åˆ†æç³»ç»Ÿ - è®¾ç½®ç•Œé¢
"""

import streamlit as st
import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

from config_manager import config

def save_config(section, key, value):
    """ä¿å­˜å•ä¸ªé…ç½®é¡¹åˆ°é…ç½®æ–‡ä»¶"""
    config.set(f'{section}.{key}', value)
    config.save()
    return True

def main():
    """æ˜¾ç¤ºè®¾ç½®ç•Œé¢"""
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    with st.container():
        st.subheader("OpenAI API è®¾ç½®")
        
        # API åŸºæœ¬è®¾ç½®
        api_key = st.text_input(
            "API Key", 
            value=config.get('LLM_OPENAI.API_KEY', ''),
            type="password",
            help="è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥"
        )
        
        base_url = st.text_input(
            "API Base URL", 
            value=config.get('LLM_OPENAI.BASE_URL', 'https://api.openai.com/v1'),
            help="è¾“å…¥APIåŸºç¡€URLï¼Œä½¿ç”¨æ›¿ä»£æœåŠ¡æ—¶éœ€è¦ä¿®æ”¹"
        )
        
        # æ¨¡å‹è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            analysis_model = st.text_input(
                "åˆ†ææ¨¡å‹", 
                value=config.get('LLM_OPENAI.DEFAULT_MODEL', 'gpt-4o'),
                help="ç”¨äºè¯¦ç»†åˆ†æçš„é«˜çº§æ¨¡å‹"
            )
        
        with col2:
            inference_model = st.text_input(
                "æ¨ç†æ¨¡å‹", 
                value=config.get('LLM_OPENAI.INFERENCE_MODEL', 'gpt-4o-mini'),
                help="ç”¨äºå¿«é€Ÿæ¨ç†çš„è½»é‡æ¨¡å‹"
            )
        
        # é«˜çº§è®¾ç½®
        with st.expander("é«˜çº§è®¾ç½®", expanded=False):
            timeout = st.number_input(
                "è¶…æ—¶æ—¶é—´(ç§’)", 
                min_value=10, 
                max_value=300, 
                value=int(config.get('LLM_OPENAI.TIMEOUT', 60)),
                help="APIè¯·æ±‚è¶…æ—¶æ—¶é—´"
            )
            
            max_retries = st.number_input(
                "æœ€å¤§é‡è¯•æ¬¡æ•°", 
                min_value=0, 
                max_value=10, 
                value=int(config.get('LLM_OPENAI.MAX_RETRIES', 3)),
                help="APIè¯·æ±‚å¤±è´¥æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°"
            )
            
            temperature = st.slider(
                "æ¸©åº¦å‚æ•°", 
                min_value=0.0, 
                max_value=2.0, 
                value=float(config.get('LLM_OPENAI.DEFAULT_TEMPERATURE', 0.7)),
                step=0.1,
                help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šæœ‰åˆ›æ„ï¼Œå€¼è¶Šä½è¶Šç¡®å®š"
            )
        
        # ç¼“å­˜è®¾ç½®
        with st.expander("ç¼“å­˜è®¾ç½®", expanded=False):
            enable_cache = st.toggle(
                "å¯ç”¨ç¼“å­˜", 
                value=config.get('LLM_CACHE.ENABLE_CACHE', False),
                help="æ˜¯å¦å¯ç”¨APIå“åº”ç¼“å­˜"
            )
            
            cache_ttl = st.number_input(
                "ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)", 
                min_value=60, 
                max_value=86400, 
                value=int(config.get('LLM_CACHE.CACHE_TTL', 3600)),
                help="ç¼“å­˜æ•°æ®çš„æœ‰æ•ˆæœŸ"
            )
        
        # ä¿å­˜æŒ‰é’®
        if st.button("ğŸ’¾ ä¿å­˜è®¾ç½®", type="primary"):
            try:
                save_config('LLM_OPENAI', 'API_KEY', api_key)
                save_config('LLM_OPENAI', 'BASE_URL', base_url)
                save_config('LLM_OPENAI', 'DEFAULT_MODEL', analysis_model)
                save_config('LLM_OPENAI', 'INFERENCE_MODEL', inference_model)
                
                save_config('LLM_OPENAI', 'TIMEOUT', timeout)
                save_config('LLM_OPENAI', 'MAX_RETRIES', max_retries)
                save_config('LLM_OPENAI', 'DEFAULT_TEMPERATURE', temperature)
                
                save_config('LLM_CACHE', 'ENABLE_CACHE', enable_cache)
                save_config('LLM_CACHE', 'CACHE_TTL', cache_ttl)
                
                st.success("è®¾ç½®å·²ä¿å­˜ï¼")
            except Exception as e:
                st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
    
    
    st.subheader("æµ‹è¯•è¿æ¥")
    if st.button("ğŸ”„ æµ‹è¯•APIè¿æ¥"):
        with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
            try:
                from llm.openai_client import OpenAIClient
                
                client = OpenAIClient(api_key=api_key)
                response = client.ask("è¿™æ˜¯ä¸€ä¸ªAPIè¿æ¥æµ‹è¯•ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'", model_type="inference")
                
                if "è¿æ¥æˆåŠŸ" in response:
                    st.success(f"APIè¿æ¥æµ‹è¯•æˆåŠŸï¼å“åº”ï¼š{response}")
                else:
                    st.warning(f"APIè¿æ¥æˆåŠŸä½†å“åº”ä¸ç¬¦åˆé¢„æœŸï¼š{response}")
            except Exception as e:
                st.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼š{str(e)}")


    with st.container():
        st.subheader("ç”¨æˆ·ç”»åƒ")
        user_profile = st.text_area(
            "è¯·æè¿°æ‚¨çš„ç”¨æˆ·ç”»åƒ",
            value=config.get('USER_PROFILE.RAW', ''),
            placeholder="ä¾‹å¦‚ï¼š\næ“…é•¿é¢†åŸŸï¼šç§‘æŠ€ã€åŒ»ç–—ï¼Œé•¿æœŸå…³æ³¨æ–°èƒ½æºæ¿å—\näº¤æ˜“ä¹ æƒ¯ï¼šåå¥½å·¦/å³ä¾§äº¤æ˜“ï¼Œé£é™©åå¥½ï¼Œå¹³å‡æŒä»“æ—¶é—´ç­‰",
            help="è¯·ç®€è¦æè¿°æ‚¨çš„æ“…é•¿é¢†åŸŸã€äº¤æ˜“ä¹ æƒ¯ç­‰ï¼Œæœ‰åŠ©äºç³»ç»Ÿæ›´å¥½åœ°ç†è§£æ‚¨çš„éœ€æ±‚"
        )
        
        common_mistakes_options = [
            "è¸ç©º", "å¥—ç‰¢", "å–é£", "è¿½é«˜æ€è·Œ", "é¢‘ç¹æ“ä½œ", "é‡ä»“å•ä¸€æ ‡çš„", "æ­¢æŸä¸åšå†³", "ç›²ç›®è·Ÿé£", "æƒ…ç»ªåŒ–äº¤æ˜“", "è¡Œæƒ…ä¸å¥½æ—¶å›é¿å…³æ³¨"
        ]
        user_mistakes = st.multiselect(
            "å¸¸çŠ¯çš„é”™è¯¯ï¼ˆå¯å¤šé€‰ï¼‰",
            options=common_mistakes_options,
            default=config.get('USER_PROFILE.MISTAKES', []),
            help="è¯·é€‰æ‹©æ‚¨åœ¨æŠ•èµ„è¿‡ç¨‹ä¸­å¸¸è§çš„é”™è¯¯ï¼Œæœ‰åŠ©äºç³»ç»Ÿä¸ªæ€§åŒ–åˆ†æå»ºè®®"
        )
        
        if st.button("ğŸ’¾ ä¿å­˜ç”¨æˆ·ç”»åƒ", key="save_user_profile", type="primary"):
            try:
                save_config('USER_PROFILE', 'RAW', user_profile)
                save_config('USER_PROFILE', 'MISTAKES', user_mistakes)
                st.success("ç”¨æˆ·ç”»åƒå·²ä¿å­˜ï¼")
            except Exception as e:
                st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")


    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            <small>XY Stock é…ç½®ç®¡ç† | é‡å¯åº”ç”¨åè®¾ç½®ç”Ÿæ•ˆ</small>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
