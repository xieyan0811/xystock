"""
XY Stock è‚¡ç¥¨åˆ†æç³»ç»Ÿ - è®¾ç½®ç•Œé¢
"""

import streamlit as st
import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config_manager import config

def save_config(section, key, value):
    """ä¿å­˜å•ä¸ªé…ç½®é¡¹åˆ°é…ç½®æ–‡ä»¶"""
    config.set(f'{section}.{key}', value)
    config.save()
    return True

def main():
    """æ˜¾ç¤ºè®¾ç½®ç•Œé¢"""
    st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    with st.container():
        st.subheader("OpenAI API è®¾ç½®")
        
        # API åŸºæœ¬è®¾ç½®
        api_key = st.text_input(
            "API Key", 
            value=config.get('LLM_OPENAI.API_KEY', ''),
            type="password",
            help="è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥"
        )
        
        base_url = st.text_input(
            "API Base URL", 
            value=config.get('LLM_OPENAI.BASE_URL', 'https://api.openai.com/v1'),
            help="è¾“å…¥APIåŸºç¡€URLï¼Œä½¿ç”¨æ›¿ä»£æœåŠ¡æ—¶éœ€è¦ä¿®æ”¹"
        )
        
        # æ¨¡å‹è®¾ç½®
        col1, col2 = st.columns(2)
        with col1:
            analysis_model = st.text_input(
                "åˆ†ææ¨¡å‹", 
                value=config.get('LLM_OPENAI.DEFAULT_MODEL', 'gpt-4o'),
                help="ç”¨äºè¯¦ç»†åˆ†æçš„é«˜çº§æ¨¡å‹"
            )
        
        with col2:
            inference_model = st.text_input(
                "æ¨ç†æ¨¡å‹", 
                value=config.get('LLM_OPENAI.INFERENCE_MODEL', 'gpt-4o-mini'),
                help="ç”¨äºå¿«é€Ÿæ¨ç†çš„è½»é‡æ¨¡å‹"
            )
        
        # é«˜çº§è®¾ç½®
        with st.expander("é«˜çº§è®¾ç½®", expanded=False):
            timeout = st.number_input(
                "è¶…æ—¶æ—¶é—´(ç§’)", 
                min_value=10, 
                max_value=300, 
                value=int(config.get('LLM_OPENAI.TIMEOUT', 60)),
                help="APIè¯·æ±‚è¶…æ—¶æ—¶é—´"
            )
            
            max_retries = st.number_input(
                "æœ€å¤§é‡è¯•æ¬¡æ•°", 
                min_value=0, 
                max_value=10, 
                value=int(config.get('LLM_OPENAI.MAX_RETRIES', 3)),
                help="APIè¯·æ±‚å¤±è´¥æ—¶æœ€å¤§é‡è¯•æ¬¡æ•°"
            )
            
            temperature = st.slider(
                "æ¸©åº¦å‚æ•°", 
                min_value=0.0, 
                max_value=2.0, 
                value=float(config.get('LLM_OPENAI.DEFAULT_TEMPERATURE', 0.7)),
                step=0.1,
                help="æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šæœ‰åˆ›æ„ï¼Œå€¼è¶Šä½è¶Šç¡®å®š"
            )
        
        # ç¼“å­˜è®¾ç½®
        with st.expander("ç¼“å­˜è®¾ç½®", expanded=False):
            enable_cache = st.toggle(
                "å¯ç”¨ç¼“å­˜", 
                value=config.get('LLM_CACHE.ENABLE_CACHE', False),
                help="æ˜¯å¦å¯ç”¨APIå“åº”ç¼“å­˜"
            )
            
            cache_ttl = st.number_input(
                "ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)", 
                min_value=60, 
                max_value=86400, 
                value=int(config.get('LLM_CACHE.CACHE_TTL', 3600)),
                help="ç¼“å­˜æ•°æ®çš„æœ‰æ•ˆæœŸ"
            )
        
        # ä¿å­˜æŒ‰é’®
        if st.button("ğŸ’¾ ä¿å­˜è®¾ç½®", type="primary"):
            try:
                # ä¿å­˜åŸºæœ¬è®¾ç½®
                save_config('LLM_OPENAI', 'API_KEY', api_key)
                save_config('LLM_OPENAI', 'BASE_URL', base_url)
                save_config('LLM_OPENAI', 'DEFAULT_MODEL', analysis_model)
                save_config('LLM_OPENAI', 'INFERENCE_MODEL', inference_model)
                
                # ä¿å­˜é«˜çº§è®¾ç½®
                save_config('LLM_OPENAI', 'TIMEOUT', timeout)
                save_config('LLM_OPENAI', 'MAX_RETRIES', max_retries)
                save_config('LLM_OPENAI', 'DEFAULT_TEMPERATURE', temperature)
                
                # ä¿å­˜ç¼“å­˜è®¾ç½®
                save_config('LLM_CACHE', 'ENABLE_CACHE', enable_cache)
                save_config('LLM_CACHE', 'CACHE_TTL', cache_ttl)
                
                st.success("è®¾ç½®å·²ä¿å­˜ï¼")
            except Exception as e:
                st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•è¿æ¥åŒºåŸŸ
    st.subheader("æµ‹è¯•è¿æ¥")
    if st.button("ğŸ”„ æµ‹è¯•APIè¿æ¥"):
        with st.spinner("æ­£åœ¨æµ‹è¯•è¿æ¥..."):
            try:
                from llm.openai_client import OpenAIClient
                
                # ä½¿ç”¨ä¸´æ—¶å®¢æˆ·ç«¯æµ‹è¯•è¿æ¥
                client = OpenAIClient(api_key=api_key)
                response = client.ask("è¿™æ˜¯ä¸€ä¸ªAPIè¿æ¥æµ‹è¯•ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'", model_type="inference")
                
                if "è¿æ¥æˆåŠŸ" in response:
                    st.success(f"APIè¿æ¥æµ‹è¯•æˆåŠŸï¼å“åº”ï¼š{response}")
                else:
                    st.warning(f"APIè¿æ¥æˆåŠŸä½†å“åº”ä¸ç¬¦åˆé¢„æœŸï¼š{response}")
            except Exception as e:
                st.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼š{str(e)}")
    
    # é¡µé¢åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            <small>XY Stock é…ç½®ç®¡ç† | é‡å¯åº”ç”¨åè®¾ç½®ç”Ÿæ•ˆ</small>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
